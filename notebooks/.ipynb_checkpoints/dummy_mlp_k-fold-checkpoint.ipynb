{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc41962-d475-416a-b7b5-4702e51081a9",
   "metadata": {},
   "source": [
    "Typically, you split your data into training and test sets. You train your model with the training set then evaluate the result with test set. But you evaluated the model only once and you are not sure your good result is by luck or not. You want to evaluate the model multiple times so you can be more confident about the model's generalizability and prevent overfitting.\n",
    "\n",
    "***K*-fold cross-validation:** \n",
    "- Shuffle the dataset randomly\n",
    "- Split the dataset into *k* subsets (folds), let's say 3\n",
    "- Train the model on *k*-1 folds, validate on the remaining fold, and repeat the process *k* times\n",
    "  - Model 1: trained on Folds 1 and 2, tested on Fold 3, save the evaluation score\n",
    "  - Model 2: trained on Folds 2 and 3, tested on Fold 1, save the evaluation score\n",
    "  - Model 3: trained on Folds 3 and 1, tested on Fold 2, save the evaluation score\n",
    "- Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbd92a-86f5-4c3b-bf38-8e9ce23d439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import proplot as pplt\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from torch.utils.data import TensorDataset,DataLoader,Subset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a4d402-01d5-424b-a82b-09457ba28e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTSIZE    = 0.2\n",
    "RANDOMSTATE = 42\n",
    "NSPLITS     = 6\n",
    "BATCHSIZE   = 322\n",
    "INPUTSIZE   = 1\n",
    "OUTPUTSIZE  = 1\n",
    "HIDDENSIZE  = 64\n",
    "ACTIVATION  = torch.nn.ReLU()\n",
    "LEARNING    = 0.005\n",
    "EPOCHS      = 6\n",
    "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d27baf5-6a6a-4fc0-8004-d867ab352bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename,xname,yname,testsize=TESTSIZE,randomstate=RANDOMSTATE):\n",
    "    filedir = '/global/cfs/cdirs/m4334/sferrett/monsoon-pod/data/processed'\n",
    "    data = xr.open_dataset(f'{filedir}/{filename}')\n",
    "    x = data[xname].load()\n",
    "    y = data[yname].load()\n",
    "    timeidxs = np.arange(x.time.size)\n",
    "    trainidxs,testidxs = train_test_split(timeidxs,test_size=testsize,random_state=randomstate)\n",
    "    xtrain = x[trainidxs]\n",
    "    ytrain = y[trainidxs]\n",
    "    xtest  = x[testidxs]\n",
    "    ytest  = y[testidxs]\n",
    "    return xtrain,ytrain,xtest,ytest\n",
    "\n",
    "def normalize(array,mean=None,std=None):\n",
    "    if mean is None:\n",
    "        mean = np.mean(array)\n",
    "    if std is None:\n",
    "        std  = np.std(array)\n",
    "    return (array-mean)/std,mean,std\n",
    "\n",
    "def preprocess(x,y,training=True,normparams=None):\n",
    "    xarray = x.values.flatten().astype(np.float64)\n",
    "    yarray = y.values.flatten().astype(np.float64)\n",
    "    if training:\n",
    "        xnorm,xmean,xstd = normalize(xarray)\n",
    "        ynorm,ymean,ystd = normalize(yarray)\n",
    "        normparams = {'xmean':xmean,'xstd':xstd,'ymean':ymean,'ystd':ystd}\n",
    "    else:\n",
    "        if normparams is None:\n",
    "            raise ValueError(\"'normparams' must be provided for validation and test sets.\")\n",
    "        xnorm,_,_ = normalize(xarray,normparams['xmean'],normparams['xstd'])\n",
    "        ynorm,_,_ = normalize(yarray,normparams['ymean'],normparams['ystd'])\n",
    "    xtensor = torch.FloatTensor(xnorm)\n",
    "    ytensor = torch.FloatTensor(ynorm)\n",
    "    return (xtensor,ytensor,normparams) if training else (xtensor,ytensor)\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self,inputsize,hiddensize,outputsize,activation):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(inputsize,hiddensize),\n",
    "            activation,\n",
    "            torch.nn.Linear(hiddensize,hiddensize),\n",
    "            activation,\n",
    "            torch.nn.Linear(hiddensize,outputsize))\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train(model,dataloader,criterion,optimizer,device):\n",
    "    model.train()\n",
    "    epochloss = 0\n",
    "    for batchinputs,batchtargets in dataloader:\n",
    "        batchinputs,batchtargets = batchinputs.to(device),batchtargets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        batchoutputs = model(batchinputs)\n",
    "        loss    = criterion(batchoutputs,batchtargets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochloss += loss.item()\n",
    "    return epochloss/len(dataloader)\n",
    "\n",
    "def evaluate(model,dataloader,criterion,device):\n",
    "    model.eval()\n",
    "    totalloss  = 0\n",
    "    alltargets = []\n",
    "    alloutputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs,targets in dataloader:\n",
    "            inputs,targets = inputs.to(device),targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss    = criterion(outputs,targets)\n",
    "            totalloss += loss.item()\n",
    "            alltargets.extend(targets.cpu().numpy())\n",
    "            alloutputs.extend(outputs.cpu().numpy())\n",
    "    r2   = r2_score(alltargets,alloutputs)\n",
    "    rmse = np.sqrt(mean_squared_error(alltargets,alloutputs))\n",
    "    return totalloss/len(dataloader),r2,rmse,np.array(alltargets),np.array(alloutputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7b4628-92b3-40c8-a696-42dca97ca7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,ytrain,xtest,ytest = load(filename='LR_ERA5_IMERG_pr_bl_terms.nc',xname='subsat',yname='bl')\n",
    "xtraintensor,ytraintensor,normparams = preprocess(xtrain,ytrain,training=True,normparams=False)\n",
    "xtesttensor,ytesttensor   = preprocess(xtest,ytest,training=False,normparams=normparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4846679b-02b3-4a0f-a075-9a17748e8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = TensorDataset(xtraintensor.unsqueeze(1),ytraintensor.unsqueeze(1))\n",
    "testdataset  = TensorDataset(xtesttensor.unsqueeze(1),ytesttensor.unsqueeze(1))\n",
    "trainloader  = DataLoader(traindataset,batch_size=BATCHSIZE,shuffle=True)\n",
    "testloader   = DataLoader(testdataset,batch_size=BATCHSIZE,shuffle=False)\n",
    "kfold        = KFold(n_splits=NSPLITS,shuffle=True,random_state=RANDOMSTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "078abb9b-c082-4a72-8300-fa59de54753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/6 ________________________________________\n",
      "Epoch 1/6 - Train Loss: 0.0363, Val Loss: 0.0360\n",
      "Epoch 2/6 - Train Loss: 0.0352, Val Loss: 0.0350\n",
      "Epoch 3/6 - Train Loss: 0.0351, Val Loss: 0.0347\n",
      "Epoch 4/6 - Train Loss: 0.0351, Val Loss: 0.0350\n",
      "Epoch 5/6 - Train Loss: 0.0350, Val Loss: 0.0371\n",
      "Epoch 6/6 - Train Loss: 0.0350, Val Loss: 0.0349\n",
      "Fold 2/6 ________________________________________\n",
      "Epoch 1/6 - Train Loss: 0.0362, Val Loss: 0.0348\n",
      "Epoch 2/6 - Train Loss: 0.0353, Val Loss: 0.0350\n",
      "Epoch 3/6 - Train Loss: 0.0352, Val Loss: 0.0350\n",
      "Epoch 4/6 - Train Loss: 0.0351, Val Loss: 0.0347\n",
      "Epoch 5/6 - Train Loss: 0.0350, Val Loss: 0.0347\n",
      "Epoch 6/6 - Train Loss: 0.0349, Val Loss: 0.0347\n",
      "Fold 3/6 ________________________________________\n",
      "Epoch 1/6 - Train Loss: 0.0367, Val Loss: 0.0349\n",
      "Epoch 2/6 - Train Loss: 0.0352, Val Loss: 0.0352\n",
      "Epoch 3/6 - Train Loss: 0.0351, Val Loss: 0.0358\n",
      "Epoch 4/6 - Train Loss: 0.0351, Val Loss: 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ffb24060190>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/homes/s/sferrett/.conda/envs/monsoon-sr/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results        = []\n",
    "alltrainlosses = []\n",
    "allvallosses   = []\n",
    "allyactual     = []\n",
    "allypred       = []\n",
    "for fold,(trainidx,validx) in enumerate(kfold.split(traindataset)):\n",
    "    print(f'Fold {fold+1}/{NSPLITS} ________________________________________')\n",
    "    trainsubset = Subset(traindataset,trainidx)\n",
    "    valsubset   = Subset(traindataset,validx)\n",
    "    trainloader = DataLoader(trainsubset,batch_size=BATCHSIZE,shuffle=True)\n",
    "    valloader   = DataLoader(valsubset,batch_size=BATCHSIZE,shuffle=False)\n",
    "    model     = MLP(INPUTSIZE,HIDDENSIZE,OUTPUTSIZE,ACTIVATION).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    trainlosses = []\n",
    "    vallosses   = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        trainloss = train(model,trainloader,criterion,optimizer,DEVICE)\n",
    "        valloss,valr2,valrmse,_,_ = evaluate(model,valloader,criterion,DEVICE)\n",
    "        trainlosses.append(trainloss)\n",
    "        vallosses.append(valloss)\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} - Train Loss: {trainloss:.4f}, Val Loss: {valloss:.4f}')\n",
    "    alltrainlosses.append(trainlosses)\n",
    "    allvallosses.append(vallosses)\n",
    "    results.append({'fold':fold,'loss':valloss,'r2':valr2,'rmse':valrmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647e1a4-93d9-48ac-b69e-04429529beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgloss = np.mean([r['loss'] for r in results])\n",
    "avgr2   = np.mean([r['r2'] for r in results])\n",
    "avgrmse = np.mean([r['rmse'] for r in results])\n",
    "print(f'Average Cross-Validation Results:')\n",
    "print(f'  Average Loss: {avgloss:.4f}')\n",
    "print(f'  Average R2: {avgr2:.4f}')\n",
    "print(f'  Average RMSE: {avgrmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb33aa8-88c6-4e3a-b828-1bed685942c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red6','orange6','yellow6','green6','blue6','violet6']\n",
    "fig,ax = pplt.subplots(nrows=1,ncols=1,refwidth=5,refheight=2)\n",
    "ax.format(suptitle='Training and Validation Losses',xlabel='Epoch',xlim=(0,5),xticks=1,ylabel='Loss')\n",
    "for fold,color in zip(range(NSPLITS),colors):\n",
    "    ax.plot(alltrainlosses[fold],color=color,linestyle='--',linewidth=1,label=f'Fold {fold+1} Training')\n",
    "    ax.plot(allvallosses[fold],color=color,linewidth=1,label=f'Fold {fold+1} Validation')\n",
    "ax.legend(loc='r',ncols=1)\n",
    "pplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66d280-c4e2-4e3a-b9c9-9e02aa67c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloss,testr2,testrmse,testtargets,testpreds = evaluate(model,testloader,criterion,DEVICE)\n",
    "print(f'Test Results:')\n",
    "print(f'  Loss: {testloss:.4f}, R2: {testr2:.4f}, RMSE: {testrmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29729e-4ba7-494d-8f24-288239c951c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(normtensor,mean,std):\n",
    "    return np.array(normtensor)*std+mean\n",
    "\n",
    "ytrue = denormalize(testtargets,normparams['ymean'],normparams['ystd'])\n",
    "ypred = denormalize(testpreds,normparams['ymean'],normparams['ystd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426ecf32-99bb-4aff-b765-c2eeed703578",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = pplt.subplots(nrows=1,ncols=2,refwidth=2,share=False)\n",
    "axs[0].format(title='Actual vs. Predicted $\\mathit{B_L}$',xlabel='Actual $\\mathit{B_L}$ (m/s$^2$)',ylabel='Predicted $\\mathit{B_L}$ (m/s$^2$)')\n",
    "axs[1].format(title='Actual vs. Predicted Histograms of $\\mathit{B_L}$',xlabel='$\\mathit{B_L}$ (m/s$^2$)',ylabel='Count',yscale='log',yformatter='log')\n",
    "axs[0].scatter(ytrue,ypred,color='cyan6',marker='.',markersize=10,alpha=0.5)\n",
    "axs[0].plot([min(min(ytrue),min(ypred)),max(max(ytrue),max(ypred))],[min(min(ytrue),min(ypred)),max(max(ytrue),max(ypred))],'k--')\n",
    "axs[0].text(0.05,0.95,f'R² = {r2_score(ytrue,ypred):.3f}',color='cyan9',transform=axs[0].transAxes,verticalalignment='top',horizontalalignment='left')\n",
    "axs[1].hist(ytrue,bins=50,filled=True,facecolor='none',edgecolor='k',linewidth=1.5,label='Actual')\n",
    "axs[1].hist(ypred,bins=50,filled=True,color='cyan6',alpha=0.5,label='Predicted')\n",
    "axs[1].legend(loc='ul',ncols=1)\n",
    "pplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monsoon-sr",
   "language": "python",
   "name": "monsoon-sr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
